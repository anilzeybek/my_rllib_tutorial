{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi everyone,\n",
    "\n",
    "In this tutorial series, I want to show you how to use RLlib to train your agents ranging from OpenAI gym environments to external robotics simulators, parallelize your training setup efficiently, and customize the behavior of your agents so that you can extend the algorithms however you want.\n",
    "\n",
    "But in this first tutorial, letâ€™s start from scratch and try to train a vanilla DQN agent for the LunarLander-v2 environment from the OpenAI gym library.\n",
    "\n",
    "First of all, let's start with installing the ray library with RLlib components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: ray[rllib] in /home/anil/.local/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs in /usr/lib/python3/dist-packages (from ray[rllib]) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/lib/python3/dist-packages (from ray[rllib]) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: click>=7.0 in /usr/lib/python3/dist-packages (from ray[rllib]) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (3.6.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<=1.43.0,>=1.28.1 in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (1.43.0)\n",
      "Requirement already satisfied, skipping upgrade: msgpack<2.0.0,>=1.0.0 in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.15.3 in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (3.19.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16; python_version < \"3.9\" in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (1.22.3)\n",
      "Requirement already satisfied, skipping upgrade: redis>=3.5.0 in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (4.1.4)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema in /usr/lib/python3/dist-packages (from ray[rllib]) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: lz4; extra == \"rllib\" in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy; extra == \"rllib\" in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: requests; extra == \"rllib\" in /usr/lib/python3/dist-packages (from ray[rllib]) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: gym<0.22; extra == \"rllib\" in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (0.21.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib!=3.4.3; extra == \"rllib\" in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (3.5.1)\n",
      "Requirement already satisfied, skipping upgrade: dm-tree; extra == \"rllib\" in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (0.1.6)\n",
      "Requirement already satisfied, skipping upgrade: tabulate; extra == \"rllib\" in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (0.8.9)\n",
      "Requirement already satisfied, skipping upgrade: pandas; extra == \"rllib\" in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboardX>=1.9; extra == \"rllib\" in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (2.5)\n",
      "Requirement already satisfied, skipping upgrade: scikit-image; extra == \"rllib\" in /home/anil/.local/lib/python3.8/site-packages (from ray[rllib]) (0.19.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.2 in /usr/lib/python3/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray[rllib]) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: deprecated>=1.2.3 in /home/anil/.local/lib/python3.8/site-packages (from redis>=3.5.0->ray[rllib]) (1.2.13)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.4 in /home/anil/.local/lib/python3.8/site-packages (from redis>=3.5.0->ray[rllib]) (21.3)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle>=1.2.0 in /home/anil/.local/lib/python3.8/site-packages (from gym<0.22; extra == \"rllib\"->ray[rllib]) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.2.1 in /home/anil/.local/lib/python3.8/site-packages (from matplotlib!=3.4.3; extra == \"rllib\"->ray[rllib]) (3.0.7)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in /home/anil/.local/lib/python3.8/site-packages (from matplotlib!=3.4.3; extra == \"rllib\"->ray[rllib]) (9.0.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7 in /home/anil/.local/lib/python3.8/site-packages (from matplotlib!=3.4.3; extra == \"rllib\"->ray[rllib]) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: fonttools>=4.22.0 in /home/anil/.local/lib/python3.8/site-packages (from matplotlib!=3.4.3; extra == \"rllib\"->ray[rllib]) (4.29.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /home/anil/.local/lib/python3.8/site-packages (from matplotlib!=3.4.3; extra == \"rllib\"->ray[rllib]) (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /home/anil/.local/lib/python3.8/site-packages (from matplotlib!=3.4.3; extra == \"rllib\"->ray[rllib]) (1.3.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2020.1 in /home/anil/.local/lib/python3.8/site-packages (from pandas; extra == \"rllib\"->ray[rllib]) (2021.3)\n",
      "Requirement already satisfied, skipping upgrade: tifffile>=2019.7.26 in /home/anil/.local/lib/python3.8/site-packages (from scikit-image; extra == \"rllib\"->ray[rllib]) (2022.2.9)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=1.1.1 in /home/anil/.local/lib/python3.8/site-packages (from scikit-image; extra == \"rllib\"->ray[rllib]) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.4.1 in /home/anil/.local/lib/python3.8/site-packages (from scikit-image; extra == \"rllib\"->ray[rllib]) (2.16.1)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.2 in /home/anil/.local/lib/python3.8/site-packages (from scikit-image; extra == \"rllib\"->ray[rllib]) (2.7.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /home/anil/.local/lib/python3.8/site-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[rllib]) (1.13.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"ray[rllib]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import our DQN agent and gym library as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RLlib, each algorithm we will be using will be a subclass of the Trainer class, as in this DQN example. Now we are ready select the environment and instantiate our agent. For instantiating the `Trainer` subclasses in RLlib, we give config dictionaries. With this config dictionaries, we define the environment we want to use and specify hyperparameters like learning rate and number of layers in the model neural network.\n",
    "\n",
    "For our first tutorial, let's select the \"CartPole-v1\" environment since it is easy to train on. Note that we didn't specify the environment in config by the class name, instead we have just gived the name of the environment. The reason is that RLlib can understand the default gym environments like that. But in future tutorials where we use our custom environments, we will use give the class name.\n",
    "\n",
    "By default, DQNTrainer uses learning rate of 5e-4 and 1 hidden layer with 256 neurons. Let's change learning rate to 1e-3 and have 2 hidden layer with 128 neurons each. As I've already said, we will use `torch` for deep learning framework so we specify it, otherwise RLlib will try to use `tensorflow`. And since we said vanilla DQN algorithm, let's turn off double_q update, dueling networks and prioritized replay. In a later tutorial, we will explicitly design a dueling neural network architecture for DQN, but now, let's continue like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        \"env\": \"CartPole-v1\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"hiddens\": [64],\n",
    "        \"double_q\": False,\n",
    "        \"dueling\": False,\n",
    "        \"prioritized_replay\": False,\n",
    "        \"framework\": \"torch\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train! Let's train the agent for 100 iterations, with each iteration is 1000 steps. This 1000 comes from the `timesteps_per_iteration` config, which is 1000 by default. If you dig into the default configs, you will see that epsilon-greedy action selection is used for exploration and target Q network updated every 500 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1, mean score: 18.884615384615383\n",
      "iter: 2, mean score: 18.87\n",
      "iter: 3, mean score: 20.59\n",
      "iter: 4, mean score: 22.52\n",
      "iter: 5, mean score: 28.27\n",
      "iter: 6, mean score: 35.47\n",
      "iter: 7, mean score: 43.25\n",
      "iter: 8, mean score: 52.44\n",
      "iter: 9, mean score: 59.91\n",
      "iter: 10, mean score: 66.98\n",
      "iter: 11, mean score: 76.04\n",
      "iter: 12, mean score: 84.11\n",
      "iter: 13, mean score: 92.71\n",
      "iter: 14, mean score: 101.56\n",
      "iter: 15, mean score: 108.44\n",
      "iter: 16, mean score: 116.5\n",
      "iter: 17, mean score: 125.86\n",
      "iter: 18, mean score: 133.22\n",
      "iter: 19, mean score: 137.84\n",
      "iter: 20, mean score: 143.79\n",
      "iter: 21, mean score: 147.88\n",
      "iter: 22, mean score: 155.06\n",
      "iter: 23, mean score: 163.21\n",
      "iter: 24, mean score: 166.44\n",
      "iter: 25, mean score: 167.37\n",
      "iter: 26, mean score: 172.01\n",
      "iter: 27, mean score: 179.31\n",
      "iter: 28, mean score: 183.73\n",
      "iter: 29, mean score: 189.13\n",
      "iter: 30, mean score: 194.92\n",
      "iter: 31, mean score: 201.68\n",
      "iter: 32, mean score: 203.98\n",
      "iter: 33, mean score: 201.82\n",
      "iter: 34, mean score: 201.92\n",
      "iter: 35, mean score: 203.96\n",
      "iter: 36, mean score: 207.17\n",
      "iter: 37, mean score: 213.41\n",
      "iter: 38, mean score: 217.9\n",
      "iter: 39, mean score: 220.83\n",
      "iter: 40, mean score: 224.73\n",
      "iter: 41, mean score: 229.32\n",
      "iter: 42, mean score: 236.4\n",
      "iter: 43, mean score: 243.79\n",
      "iter: 44, mean score: 250.86\n",
      "iter: 45, mean score: 254.7\n",
      "iter: 46, mean score: 254.05\n",
      "iter: 47, mean score: 235.67\n",
      "iter: 48, mean score: 229.38\n",
      "iter: 49, mean score: 224.52\n",
      "iter: 50, mean score: 222.67\n",
      "iter: 51, mean score: 217.94\n",
      "iter: 52, mean score: 222.79\n",
      "iter: 53, mean score: 218.4\n",
      "iter: 54, mean score: 221.77\n",
      "iter: 55, mean score: 221.78\n",
      "iter: 56, mean score: 222.36\n",
      "iter: 57, mean score: 224.56\n",
      "iter: 58, mean score: 230.54\n",
      "iter: 59, mean score: 237.58\n",
      "iter: 60, mean score: 240.77\n",
      "iter: 61, mean score: 242.97\n",
      "iter: 62, mean score: 244.26\n",
      "iter: 63, mean score: 248.44\n",
      "iter: 64, mean score: 253.13\n",
      "iter: 65, mean score: 258.89\n",
      "iter: 66, mean score: 261.38\n",
      "iter: 67, mean score: 262.17\n",
      "iter: 68, mean score: 259.98\n",
      "iter: 69, mean score: 259.98\n",
      "iter: 70, mean score: 259.98\n",
      "iter: 71, mean score: 267.2\n",
      "iter: 72, mean score: 270.69\n",
      "iter: 73, mean score: 274.1\n",
      "iter: 74, mean score: 282.19\n",
      "iter: 75, mean score: 289.78\n",
      "iter: 76, mean score: 296.55\n",
      "iter: 77, mean score: 304.21\n",
      "iter: 78, mean score: 311.83\n",
      "iter: 79, mean score: 317.37\n",
      "iter: 80, mean score: 324.91\n",
      "iter: 81, mean score: 328.39\n",
      "iter: 82, mean score: 327.47\n",
      "iter: 83, mean score: 324.65\n",
      "iter: 84, mean score: 321.87\n",
      "iter: 85, mean score: 322.57\n",
      "iter: 86, mean score: 318.24\n",
      "iter: 87, mean score: 325.98\n",
      "iter: 88, mean score: 333.32\n",
      "iter: 89, mean score: 340.25\n",
      "iter: 90, mean score: 347.79\n",
      "iter: 91, mean score: 354.5\n",
      "iter: 92, mean score: 360.41\n",
      "iter: 93, mean score: 363.53\n",
      "iter: 94, mean score: 363.32\n",
      "iter: 95, mean score: 363.32\n",
      "iter: 96, mean score: 365.4\n",
      "iter: 97, mean score: 365.4\n",
      "iter: 98, mean score: 365.4\n",
      "iter: 99, mean score: 360.57\n",
      "iter: 100, mean score: 353.14\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    results = trainer.train()\n",
    "    print(f\"iter: {trainer.iteration}, mean score: {results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train completed! In the CartPole-v1 environment, the maximum score agent can get is 500, which we managed to get 353 mean score. But we were taking explorationary actions, now let's see what happens if we act fully greedy.\n",
    "\n",
    "For that we create the usual reinforcement learning loop and we can calculate actions for observations by `compute_single_action` method of trainer. Don't forget to specify `explore=False` so we act fully greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n",
      "500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "for _ in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action = trainer.compute_single_action(obs, explore=False)\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        score += reward\n",
    "\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have played for 10 episodes and we got 500 reward at each episode! And this brings us to the end of first episode. In the next tutorial, we will create our custom environment with continuous actions and try to solve it with DDPG algorithm."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
